## 多进程

1. 简介

   > 一个人有很多砖需要搬，他领取手套、推车各种物资（向系统申请了资源）然后开始搬砖。然而他身边有很多人，我们让这些人去帮他！（一核有难，八核围观）。于是他们做了分工，砖很快就搬完了。多进程让多个CPU核心可以一起做事，不至于只有一人干活而其他人傻站着。

2. > 我们知道一个进程占用一个CPU，现在的配置CPU一般都是4核，我们启动两个进程就是分别在两个CPU里面（两个内核）各运行一个进程，我知道进程里面才有线程，默认是一个。但是有个缺点，按照上面的说法，开两个进程占用的内存空间是开一个进程占用内存空间的2倍。CPU就占用了2个核，电脑还得干别的事儿对吧，不能冒冒失失瞎用。开的太多是不是其他程序就得等着，我们思考一下，占用这么多的内存空间，利用了多个CPU的优点为了什么？CPU是用来做什么的？没错就是用来计算的，所以在CPU密集运算的情况下建议用多进程。注意，具体要开几个进程，根据机器的实际配置和实际生产情况而定。

3. 缺点:

   多进程是指在一台计算机上，同时运行多个进程，每个进程可以独立运行，但是不能共享程序中的全局变量。多进程可以提高程序的运行效率，但是由于进程之间的隔离，容易出现资源浪费的问题。

4. 用处：

   进行高性能计算。只有多进程方案设计合理，才能加速计算。
   
5. 代码实现

```python
from multiprocessing import Process
import time


def run1():
    for i in range(5):
        time.sleep(1)
        print("run1函数执行完成")
def run2():
    for i in range(5):
        time.sleep(1)
        print("run2函数执行完成")
t1 = time.time()
if __name__ == '__main__':
    p1 = Process(target=run1)#创建子进程p1  ！！han'shu
    p2 = Process(target=run2)#创建子进程p2
    p1.start()#开启p1进程
    p2.start()
    p1.join()#阻塞等待p1子进程执行完-->主进程在等待子进程干活
    p2.join()#要这个子进程执行完后，程序才会向下继续运行
    print("子进程执行完成,继续开始主进程")
    print(time.time() - t1)
```

6. 多进程传参

   * 参数尽量传元组

   * 元组只有一个参数时，注意加`,`如：`args=(2,)`

   * ```python
     from multiprocessing import Process
     import time
     
     
     def run1(num):
         for i in range(num):
             time.sleep(1)
             print("run1函数执行完成")
     def run2(num,name):
         for i in range(num):
             time.sleep(1)
             print(f"{name}函数执行完成")
     t1 = time.time()
     if __name__ == '__main__':
         p1 = Process(target=run1,args=(5,))#创建子进程p1并传一个参数
         p2 = Process(target=run2,args=(5,"run2"))#创建子进程p2并传两个参数
         p1.start()#开启p1进程
         p2.start()
         p1.join()#阻塞等待p1子进程执行完-->主进程在等待子进程干活
         p2.join()#要这个子进程执行完后，程序才会向下继续运行
         print("子进程执行完成,继续开始主进程")
         print(time.time() - t1)
     ```

7. 子进程未修改全局变量

   ```python
   from multiprocessing import Process
   num = 2
   def run1():
       global num
       num = 1
       print("run1函数执行完成")
   if __name__ == '__main__':
       p1 = Process(target=run1)
       p1.start()
       p1.join()
       print(num) # 2
   ```

8. 进程通信队列

   * ```python
     from multiprocessing import Process,Queue
     
     def run1(que: Queue):
         que.put(1)
         que.put(2)
         que.put(3)
         print("run1函数执行完成")
     
     if __name__ == '__main__':
         que = Queue()
         p1 = Process(target=run1,args=(que,))
         p1.start()
         p1.join()
         print("主进程获取数据",que.get())
         print("主进程获取数据",que.get())
         print("主进程获取数据",que.get())
     ```

9. 进程通信字典

   ```python
   from multiprocessing import Process, Manager
   
   def run1(dic: dict):
       dic["name"] = "九问"
       dic["phone"] = 193
       print("run1函数执行完成")
   
   if __name__ == '__main__':
       dic = Manager().dict()
       p1 = Process(target=run1, args=(dic,)) 
       p1.start()
       p1.join()
       print(f"name={dic['name']}, phone={dic['phone']}")
   ```

10. 进程通信列表

    ```python
    from multiprocessing import Process, Manager
    def run1(li: list):
        li.append(1)
        li.append(2)
        print("run1函数执行完成")
    
    if __name__ == '__main__':
        li = Manager().list()
        p1 = Process(target=run1, args=(li,)) 
        p1.start()
        p1.join()
        for i in li:
            print(i)
    ```

11. 计算cpu数量

    ```python
    import multiprocessing,os
    
    print(multiprocessing.cpu_count())
    os.getpid() # 获取进程id
    ```

12. 守护进程daemon=True 主进程结束，子进程也结束

    ```python
    from multiprocessing import Process, Manager
    def run1(li: list):
        li.append(1)
        li.append(2)
        print("run1函数执行完成")
    
    if __name__ == '__main__':
        li = Manager().list()
        p1 = Process(target=run1, args=(li,)) 
        p1.daemon = True
        p1.start()
        for i in li:
            print(i) # 无
    ```

13. 进程池

    * ```python
      from multiprocessing import Pool
      import time
      def run1(li: list):
          for i in li:
              print(i)
              time.sleep(1)
          print("run1函数执行完成")
      
      if __name__ == '__main__':
          pool = Pool(processes=3)
          pool.apply_async(run1,([1,2,3],)) # 不阻塞 代码继续往下执行
          pool.close()
          pool.join()
      ```

    * ```python
      from multiprocessing import Pool
      import time
      def run1(li: list):
          for i in li:
              print(i)
              time.sleep(1)
          print("run1函数执行完成")
      
      if __name__ == '__main__':
          pool = Pool(processes=3)
          pool.apply(run1,([1,2,3],)) #阻塞，等待run1执行完成 同步
      ```

    * map_async()不阻塞

    *  map()阻塞

      > `pool.map()` 方法的结果和Python内置的 `map()` 结果是相同的，不同的是 `pool.map()` 是通过多个并行进程计算的。
      >
      > [10. 如何使用进程池 — python-parallel-programming-cookbook-cn 1.0 文档](https://python-parallel-programmning-cookbook.readthedocs.io/zh-cn/latest/chapter3/10_How_to_use_a_process_pool.html)

    

## 多线程

1. 简介

   >  一般情况下我们启动一个.py文件，就等于启动了一个进程，一个进程里面默认有一个线程工作，我们使用的多线程的意思就是在一个进程里面启用多个线程。但问题来了，为什么要使用多线程呢？我知道启动一个进程的时候需要创建一些内存空间，就相当于一间房子，我们要在这个房子里面干活，你可以想一个人就等于一个线程，你房子里面有10个人的空间跟有20个人的空间，正常情况下是不一样的，因为我们知道线程和线程之间默认是可以通信的（进程之间默认是不可以通信的，不过可以用技术实现，比如说管道）。可以多线程为了保证计算数据的正确性，所以出现了GIL锁（全局解释器锁），保证同一时间只能有一个线程在计算。GIL锁（全局解释器锁）你可以基本理解为，比如在这个房间里要算一笔账，在同一时间内只能有一个人在算这笔账，想一个问题，如果这笔账5个人就能算清楚，我需要10平米的房间就行，那为什么要请10个人，花20平米呢？所以并不是开的线程越多越好。但是，但是，但是，注意大家不用动脑筋（CPU计算）算这笔账的时候可以去干别的事（比如说5个人分工，各算一部分），比如说各自把自己算完后的结果记录在账本上以便后面对账，这个的话每个人都有自己的账本，所以多线程适合IO操作，记住了就算是适合IO操作，也不代表说人越多越好，所以这个量还是得根据实际情况而定。

2. > 一个人有与异性聊天和看剧两件事要做。单线程的她可以看完剧再去聊天，但这样子可能就没人陪她聊天了「哼，发消息不回」。我们把她看成一个CPU核心，为她开起多线程——先看一会剧，偶尔看看新消息，在两件事（线程）间来回切换。多线程：单个CPU核心可以同时做几件事，不至于卡在某一步傻等着。

3. 缺点：

   多线程是指在单个程序中，同时运行多个线程，每个线程可以独立运行，并且可以共享程序中的全局变量。多线程可以提高程序的运行效率，但是由于线程之间的共享，容易出现数据不一致的问题。（线程共享内存，同时操作全局变量时要注意上锁。进程不共享内存，故无需上锁。）

   案例：多个线程同时对全局变量进行减操作

   ```python
   import threading
   n = 200000000
   # lock = threading.Lock()
   def work():
       global n
       for i in range(10000000):
           # # 上锁
           # lock.acquire()
           n += 1
           # # 释放锁
           # lock.release()
           # with lock: # di'er'z
           #     n -= 1
   
   
   if __name__ == '__main__':
       # 两个线程共同操作n
       t1 = threading.Thread(target=work)
       t2 = threading.Thread(target=work)
       t1.start()
       t2.start()
       t1.join()
       t2.join()
       print(n) # 未上锁可能出现n与预期值不符合的情况
   ```

4. 用处：

   爬取网站信息（爬虫），等待多个用户输入

## 协程

[协程与任务 — Python 3.11.5 文档](https://docs.python.org/zh-cn/3/library/asyncio-task.html)

### 简介

> 通常我们认为线程是轻量级的进程，因此我们也把协程理解为轻量级的线程即微线程。
>
> 切换：用户态：切换不需要CPU调度，协程
>
> 核心态：线程、进程切换，需要CPU调度

通常在Python中我们进行并发编程一般都是使用多线程或者多进程来实现的，对于计算型任务由于GIL的存在我们通常使用多进程来实现，而对于IO型任务我们可以通过线程调度来让线程在执行IO任务时让出GIL，从而实现表面上的并发。其实对于IO型任务我们还有一种选择就是协程，协程是运行在单线程当中的"并发"，协程相比多线程一大优势就是省去了多线程之间的切换开销，获得了更大的运行效率。

协程，又称微线程，纤程，英文名Coroutine。协程的作用是在执行函数A时可以随时中断去执行函数B，然后中断函数B继续执行函数A（可以自由切换）。但这一过程并不是函数调用，这一整个过程看似像多线程，然而协程只有一个线程执行。

**那协程有什么优势呢？**

- 执行效率极高，因为子程序切换（函数）不是线程切换，由程序自身控制，没有切换线程的开销。所以与多线程相比，线程的数量越多，协程性能的优势越明显。
- 不需要多线程的锁机制，因为只有一个线程，也不存在同时写变量冲突，在控制共享资源时也不需要加锁，因此执行效率高很多。

> 协程可以处理IO密集型程序的效率问题，但是处理CPU密集型不是它的长处，如要充分发挥CPU利用率可以结合多进程+协程。

### asycio实现协程

python内置模块，推荐使用的方式。

```python
import time
import asyncio  # 异步IO库， 单线程实现并发 —— 协程


# async  关键字     await关键字

async def work1():  # aysnc  任务work1是异步的
    for i in range(5):
        print(f'work1:听音乐...')
        await asyncio.sleep(1)


async def work2():
    for i in range(5):
        print(f'work2:打游戏...')
        await asyncio.sleep(1)


async def main():
    task1 = asyncio.create_task(work1())  # 任务一
    task2 = asyncio.create_task(work2())  # 任务二
    await task1
    await task2


if __name__ == '__main__':
    start = time.time()
    asyncio.run(main())
    end = time.time()
    print(f'共花费时间{(end - start):.2f}秒')

```

## 进程和线程的区别

1. 进程时资源管理的基本单位，线程是调度的基本单位。
2. 切换。进程间切换开销大，线程间切换开销小。
3. 进程有独立的内存单元，线程共享进程内存。

## 爬虫实战案例-博客园

> 网址：  urls = [f"https://www.cnblogs.com/sitehome/p/{page}" for page in range(1, 11)]
>
> 爬取sitehome页字段title,summary,avatar,author

### 单进程采集

```python
import requests
from lxml import etree
import csv
import time
import decimal

# 开始时间
start_time = time.time()
table_headers = ("title", "summary", "avatar", "author")
fp = open("cnblogs.csv", "w", encoding="utf-8", newline="")
writer = csv.writer(fp)
writer.writerow(table_headers)

for page in range(1, 11):
    url = f"https://www.cnblogs.com/sitehome/p/{page}"

    text = requests.get(url).text

    ele = etree.HTML(text)
    """
    //div[@id="post_list"]/article/section
    """
    sections = ele.xpath("//div[@id='post_list']/article/section")
    for section in sections:
        try:
            title = section.xpath("./div/a/text()")[0]  # 标题

            summary = section.xpath("./div/p/text()")[1].strip("\n ...")  # 摘要
            avatar = section.xpath("./div/p//img/@src")[0]  # 摘要

            author = section.xpath("./footer/a/span/text()")[0]  # 作者

        except IndexError as e:
            summary = section.xpath("./div/p/text()")

        writer.writerow((title, summary, avatar, author))

end_time = time.time()  # 结束时间
total_time = decimal.Decimal(str(end_time)) - decimal.Decimal(str(start_time))

print("消耗时间：", total_time)  # 6.0266562

```

## 多进程采集

>  采用4个进程池

```python
import requests
from lxml import etree
import csv
import time
import decimal
from multiprocessing import Pool

"""
创建进程的数量  = CPU核心数
"""

# 开始时间
start_time = time.time()
table_headers = ("title", "summary", "avatar", "author")
fp = open("cnblogs.csv", "a", encoding="utf-8", newline="")#代码中使用追加模式来保存博客园的数据是为了防止多个进程之间相互覆盖写入的情况，确保数据不会被同时写入而造成混乱或数据丢失。这是因为在多进程并发的情况下，多个进程同时写入同一个文件可能会引发竞争条件和数据不一致的问题。

# 追加模式（"a"模式）在每次写入数据时都会将数据追加到文件的末尾，而不会影响已经存在的数据。这样可以确保每个进程都能够顺利地将爬取到的数据添加到文件中，而不会互相干扰。

# 如果使用覆盖模式（"w"模式），多个进程可能会在同一时间尝试打开文件并写入数据，这可能导致数据交叉写入、数据丢失或文件损坏等问题。因此，为了保证数据的完整性和正确性，在多进程并发写入数据时，使用追加模式是一个更安全的选择。


writer = csv.writer(fp)
writer.writerow(table_headers)

def spider(url):
    text = requests.get(url).text

    ele = etree.HTML(text)
    """
    //div[@id="post_list"]/article/section
    """
    sections = ele.xpath("//div[@id='post_list']/article/section")
    for section in sections:
        try:
            title = section.xpath("./div/a/text()")[0]  # 标题

            summary = section.xpath("./div/p/text()")[1].strip("\n ...")  # 摘要
            avatar = section.xpath("./div/p//img/@src")[0]  # 摘要

            author = section.xpath("./footer/a/span/text()")[0]  # 作者

            writer.writerow((title, summary, avatar, author))

        except IndexError as e:
            pass
if __name__ == "__main__":
    urls = [f"https://www.cnblogs.com/sitehome/p/{page}" for page in range(1, 11)]
    pool = Pool(processes=4)

    pool.map(spider, urls)
    pool.close()

    pool.join()  # 把所有进程执行完才往下走

    end_time = time.time()  # 结束时间
    total_time = decimal.Decimal(str(end_time)) - decimal.Decimal(str(start_time))

    print("消耗时间：", total_time)  # 2.8660096

```

## 多线程采集

> 线程池-12个线程

```python
import requests
from lxml import etree
import csv
import time
import decimal
from concurrent.futures import ThreadPoolExecutor

"""

每一个程序就是一个进程


线程：进程里面一个任务


是在一个进程里面使用多线程

计算型的任务使用多进程
网络IO型的任务使用多线程


线程数 = CPU 核心数 * (1 + IO 耗时/ CPU 耗时) 


"""

# 开始时间
start_time = time.time()
table_headers = ("title", "summary", "avatar", "author")
fp = open("cnblogs.csv", "a", encoding="utf-8", newline="")
writer = csv.writer(fp)
writer.writerow(table_headers)


def spider(url):
    text = requests.get(url).text

    ele = etree.HTML(text)
    """
    //div[@id="post_list"]/article/section
    """
    sections = ele.xpath("//div[@id='post_list']/article/section")
    for section in sections:
        try:
            title = section.xpath("./div/a/text()")[0]  # 标题

            summary = section.xpath("./div/p/text()")[1].strip("\n ...")  # 摘要
            avatar = section.xpath("./div/p//img/@src")[0]  # 摘要

            author = section.xpath("./footer/a/span/text()")[0]  # 作者
            writer.writerow((title, summary, avatar, author))

        except IndexError as e:
            pass


if __name__ == "__main__":
    urls = [f"https://www.cnblogs.com/sitehome/p/{page}" for page in range(1, 11)]
    pool = ThreadPoolExecutor(12)

    for url in urls:
        pool.submit(spider, url)

    pool.shutdown(True)  # 执行完所有任务才往下运行

    end_time = time.time()  # 结束时间
    total_time = decimal.Decimal(str(end_time)) - decimal.Decimal(str(start_time))

    print("消耗时间：", total_time)  # 1.2979644

```

## 异步编程-协程实现

```python
# 开始时间
import csv
import decimal
import time
from lxml import etree
import asyncio  # 异步IO库
import aiohttp  # 异步网络库,基于异步网络请求的模块


start_time = time.time()
table_headers = ("title", "summary", "avatar", "author")
fp = open("cnblogs.csv", "a", encoding="utf-8", newline="")
writer = csv.writer(fp)
writer.writerow(table_headers)


async def spider(url):
    #request.get是基于同步,必须使用异步的网络请求模块进行请求
    async with aiohttp.ClientSession() as session:
        async with  session.get(url) as response:#可用get/post,可加headers,proxy....
            document = await response.text()#text()返回字符串形式的响应数据 read():返回二进制形式的响应数据 json():返回json对象
            #获取响应数据操作之前一定要用await挂起
    ele = etree.HTML(document)
    """
    //div[@id="post_list"]/article/section
    """
    sections = ele.xpath("//div[@id='post_list']/article/section")
    for section in sections:
        try:
            title = section.xpath("./div/a/text()")[0]  # 标题

            summary = section.xpath("./div/p/text()")[1].strip("\n ...")  # 摘要
            avatar = section.xpath("./div/p//img/@src")[0]  # 摘要

            author = section.xpath("./footer/a/span/text()")[0]  # 作者

            writer.writerow((title, summary, avatar, author))

        except IndexError as e:
            pass

if __name__ == "__main__":
    urls = [f"https://www.cnblogs.com/sitehome/p/{page}" for page in range(1, 11)]
    tasks = []  # 任务池

    for url in urls:
        tasks.append(asyncio.ensure_future(spider(url)))

    # 创建事件池
    loop = asyncio.get_event_loop()
    loop.run_until_complete(asyncio.wait(tasks))  # 执行异步任务

    end_time = time.time()  # 结束时间
    total_time = decimal.Decimal(str(end_time)) - decimal.Decimal(str(start_time))

    print("消耗时间：", total_time)  # 0.4992242

```

